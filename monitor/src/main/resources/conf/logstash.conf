input{
    file{
        # 要读取的日志文件，可用*通配符匹配  例如["/tmp/log4j/*.log"]
        path=>["/tmp/log4j/test.log"]
        # 从文件读取日志的间隔，单位：秒
        stat_interval=>5
    }
}
#处理读取到的信息
filter{
    grok{
        # 日志文件中读取到的整行将被存储到"message"属性，使用正则表达式将消息拆分为多个字段
        # 验证工具 https://grokdebug.herokuapp.com/
        match=>{
            "message"=>"(?<logTime>[0-9:\-\s,]+?)\s+\[.*\[OP\]\-\[(?<uid>.+?)\]\-\[(?<name>.+?)\]\-\[(?<content>.+?)\]\-\[(?<module>.+?)\]"
        }
        # 移除不需要的属性
        remove_field=>["message"]
    }
    if "_grokparsefailure" in [tags] {
        drop{}
    }
}
# 输出日志
output{
    # 输出到kafka
    kafka{
        # kafka topic
        topic_id=>"oc"
        # kafka-server/broker地址
        bootstrap_servers=>"a0:9092,a1:9092,a2:9092"
    }
    # 输出到elasticsearch
    elasticsearch{
        # elasticsearch地址
        hosts=>"a1:9200"
    }
    # 输出到终端
    stdout{
        codec => rubydebug
    }
}